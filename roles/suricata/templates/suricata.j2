%YAML 1.1
---

# Suricata configuration file. In addition to the comments describing all
# options in this file, full documentation can be found at:
# https://redmine.openinfosecfoundation.org/projects/suricata/wiki/Suricatayaml

# NOTE: This version of the Suricata.yaml file is modified by the Emerging Threats 
# Pro Team to reflect the default assumptions on inspection depths. Comments have been
# added to note where users may tune defaults back where performance is a 
# concern. The default stance of this config is maximum detection.
# Decisions on features such as file extraction and pcap logging must be made
# locally. These are left at defaults.

# Number of packets allowed to be processed simultaneously.  Default is a
# conservative 1024. A higher number will make sure CPU's/CPU cores will be
# more easily kept busy, but may negatively impact caching.
#
# If you are using the CUDA pattern matcher (b2g_cuda below), different rules
# apply. In that case try something like 4000 or more. This is because the CUDA
# pattern matcher scans many packets in parallel.
max-pending-packets: {{ max_pending_packets }}

# Runmode the engine should use. Please check --list-runmodes to get the available
# runmodes for each packet acquisition method. Defaults to "autofp" (auto flow pinned
# load balancing).
runmode: workers

# Default pid file.
# Will use this file if no --pidfile in command options.
pid-file: /home/suricata/run/suricata-internal.pid

# Preallocated size for packet. Default is 1514 which is the classical
# size for pcap on ethernet. You should adjust this value to the highest
# packet size (MTU + hardware header) on your system.
default-packet-size: 1534

# The default logging directory.  Any log or output file will be
# placed here if its not specified with a full path name.  This can be
# overridden with the -l command line parameter.
default-log-dir: /var/log/nsm/

# Unix command socket can be used to pass commands to suricata.
# An external tool can then connect to get information from suricata
# or trigger some modification of the engine.
#unix-command: yes

# Configure the type of alert (and other) logging you would like.
outputs:

  - eve-log:
      enabled: yes
      type: file
      filename: eve.json
      types:
          - alert:
              payload: yes
              payload-printable: yes
              packet: yes
              http: yes
              tls: yes
              ssh: yes
              smtp: yes
          - stats:
              totals: yes
              threads: yes
              deltas: no
          - http:
              extended: yes
          - dns:
              query: yes
              answer: yes
          - tls:
              extended: yes
          - files:
              force-magic: yes
          - smtp:
              force-magic: yes
          - ssh
          - flow

  - stats:
      enabled: yes
      filename: stats.log
      interval: 60
      totals: yes
      threads: yes
      deltas: no
      append: no

# Magic file. The extension .mgc is added to the value here.
magic-file: /usr/share/file/magic

# af-packet support
# Set threads to > 1 to use PACKET_FANOUT support
af-packet:
  - interface: {{ cap_name }}
    # Number of receive threads (>1 will enable experimental flow pinned
    # runmode)
    threads: {{ worker_threads }}
    # Default clusterid.  AF_PACKET will load balance packets based on flow.
    # All threads/processes that will participate need to have the same
    # clusterid.
    cluster-id: {{ suricata_cluster_id }}
    # Default AF_PACKET cluster type. AF_PACKET can load balance per flow or per hash.
    # This is only supported for Linux kernel > 3.1
    # possible value are:
    #  * cluster_round_robin: round robin load balancing
    #  * cluster_flow: all packets of a given flow are send to the same socket
    #  * cluster_cpu: all packets treated in kernel by a CPU are send to the same socket
    cluster-type: cluster_flow
    # In some fragmentation case, the hash can not be computed. If "defrag" is set
    # to yes, the kernel will do the needed defragmentation before sending the packets.
    defrag: yes
    # To use the ring feature of AF_PACKET, set 'use-mmap' to yes
    use-mmap: yes
    tpacket-v3: yes
    # Ring size will be computed with respect to max_pending_packets and number
    # of threads. You can set manually the ring size in number of packets by setting
    # the following value. If you are using flow cluster-type and have really network
    # intensive single-flow you could want to set the ring-size independantly of the number
    # of threads:
    ring-size: {{ suricata_ring_size }}
    block-size: {{ suricata_block_size }}
    # On busy system, this could help to set it to yes to recover from a packet drop
    # phase. This will result in some packets (at max a ring flush) being non treated.
    #use-emergency-flush: yes
    # recv buffer size, increase value could improve performance
    # buffer-size: 32768
    # Set to yes to disable promiscuous mode
    # disable-promisc: no
    # Choose checksum verification mode for the interface. At the moment
    # of the capture, some packets may be with an invalid checksum due to
    # offloading to the network card of the checksum computation.
    # Possible values are:
    #  - kernel: use indication sent by kernel for each packet (default)
    #  - yes: checksum validation is forced
    #  - no: checksum validation is disabled
    #  - auto: suricata uses a statistical approach to detect when
    #  checksum off-loading is used.
    # Warning: 'checksum-validation' must be set to yes to have any validation
    #checksum-checks: kernel
    # BPF filter to apply to this interface. The pcap filter syntax apply here.
    #bp#f-filter: port 80 or udp
    # You can use the following variables to activate AF_PACKET tap od IPS mode.
    # If copy-mode is set to ips or tap, the traffic coming to the current
    # interface will be copied to the copy-iface interface. If 'tap' is set, the
    # copy is complete. If 'ips' is set, the packet matching a 'drop' action
    # will not be copied.

# You can specify a threshold config file by setting "threshold-file"
# to the path of the threshold config file:
threshold-file: /etc/nsm/threshold.conf

# The detection engine builds internal groups of signatures. The engine
# allow us to specify the profile to use for them, to manage memory on an
# efficient way keeping a good performance. For the profile keyword you
# can use the words "low", "medium", "high" or "custom". If you use custom
# make sure to define the values at "- custom-values" as your convenience.
# Usually you would prefer medium/high/low.
#
# "sgh mpm-context", indicates how the staging should allot mpm contexts for
# the signature groups.  "single" indicates the use of a single context for
# all the signature group heads.  "full" indicates a mpm-context for each
# group head.  "auto" lets the engine decide the distribution of contexts
# based on the information the engine gathers on the patterns from each
# group head.
#
# The option inspection-recursion-limit is used to limit the recursive calls
# in the content inspection code.  For certain payload-sig combinations, we
# might end up taking too much time in the content inspection code.
# If the argument specified is 0, the engine uses an internally defined
# default limit.  On not specifying a value, we use no limits on the recursion.
detect:
  prefilter:
    default: auto
  profile: high
  custom-values:
    toclient-groups: 100
    toserver-groups: 100
  sgh-mpm-context: single
  inspection-recursion-limit: 3000
  rule-reload: true
  grouping:
    tcp-whitelist: "{{ grouping_tcp_whitelist }}"
    udp-whitelist: "{{ grouping_udp_whitelist }}"

  # When rule-reload is enabled, sending a USR2 signal to the Suricata process
  # will trigger a live rule reload. Experimental feature, use with care.
  #- rule-reload: true
  # If set to yes, the loading of signatures will be made after the capture
  # is started. This will limit the downtime in IPS mode.
  #- delayed-detect: yes

# Suricata is multi-threaded. Here the threading can be influenced.
threading:
  # On some cpu's/architectures it is beneficial to tie individual threads
  # to specific CPU's/CPU cores. In this case all threads are tied to CPU0,
  # and each extra CPU/core has one "detect" thread.
  #
  # On Intel Core2 and Nehalem CPU's enabling this will degrade performance.
  #
  set-cpu-affinity: yes
  # Tune cpu affinity of suricata threads. Each family of threads can be bound
  # on specific CPUs.
  cpu-affinity:
    - management-cpu-set:
        cpu: {{ management_cpu_set }}  # include only these cpus in affinity settings
        mode: "exclusive"
        prio:
          default: "low"
    - detect-cpu-set:
        cpu: {{ detect_cpu_set }}
        mode: "exclusive" # run detect threads in these cpus
        prio:
          default: "high"
  #
  # By default Suricata creates one "detect" thread per available CPU/CPU core.
  # This setting allows controlling this behaviour. A ratio setting of 2 will
  # create 2 detect threads for each CPU/CPU core. So for a dual core CPU this
  # will result in 4 detect threads. If values below 1 are used, less threads
  # are created. So on a dual core CPU a setting of 0.5 results in 1 detect
  # thread being created. Regardless of the setting at a minimum 1 detect
  # thread will always be created.
  #
  detect-thread-ratio: 1.0

# Select the multi pattern algorithm you want to run for scan/search the
# in the engine. The supported algorithms are b2g, b2gc, b2gm, b3g, wumanber,
# ac and ac-gfbs.
#
# The mpm you choose also decides the distribution of mpm contexts for
# signature groups, specified by the conf - "detect-engine.sgh-mpm-context".
# Selecting "ac" as the mpm would require "detect-engine.sgh-mpm-context"
# to be set to "single", because of ac's memory requirements, unless the
# ruleset is small enough to fit in one's memory, in which case one can
# use "full" with "ac".  Rest of the mpms can be run in "full" mode.
#
# There is also a CUDA pattern matcher (only available if Suricata was
# compiled with --enable-cuda: b2g_cuda. Make sure to update your
# max-pending-packets setting above as well if you use b2g_cuda.

mpm-algo: hs

spm-algo: hs

# The memory settings for hash size of these algorithms can vary from lowest
# (2048) - low (4096) - medium (8192) - high (16384) - higher (32768) - max
# (65536). The bloomfilter sizes of these algorithms can vary from low (512) -
# medium (1024) - high (2048).
#
# For B2g/B3g algorithms, there is a support for two different scan/search
# algorithms. For B2g the scan algorithms are B2gScan & B2gScanBNDMq, and
# search algorithms are B2gSearch & B2gSearchBNDMq. For B3g scan algorithms
# are B3gScan & B3gScanBNDMq, and search algorithms are B3gSearch &
# B3gSearchBNDMq.
#
# For B2g the different scan/search algorithms and, hash and bloom
# filter size settings. For B3g the different scan/search algorithms and, hash
# and bloom filter size settings. For wumanber the hash and bloom filter size
# settings.

pattern-matcher:
  - b2gc:
      search-algo: B2gSearchBNDMq
      hash-size: max
      bf-size: high
  - b2gm:
      search-algo: B2gSearchBNDMq
      hash-size: max
      bf-size: high
  - b2g:
      search-algo: B2gSearchBNDMq
      hash-size: max
      bf-size: high
  - b3g:
      search-algo: B3gSearchBNDMq
      hash-size: max
      bf-size: high
  - wumanber:
      hash-size: max
      bf-size: high

# Defrag settings:

defrag:
  memcap: {{ defrag_memcap }}
  hash-size: {{ defrag_hash_size }}
  trackers: {{ defrag_trackers }} # number of defragmented flows to follow
  max-frags: {{ defrag_max_frags }} # number of fragments to keep (higher than trackers)
  prealloc: yes
  timeout: {{ defrag_timeout }}

# Flow settings:
# By default, the reserved memory (memcap) for flows is 32MB. This is the limit
# for flow allocation inside the engine. You can change this value to allow
# more memory usage for flows.
# The hash-size determine the size of the hash used to identify flows inside
# the engine, and by default the value is 65536.
# At the startup, the engine can preallocate a number of flows, to get a better
# performance. The number of flows preallocated is 10000 by default.
# emergency-recovery is the percentage of flows that the engine need to
# prune before unsetting the emergency state. The emergency state is activated
# when the memcap limit is reached, allowing to create new flows, but
# prunning them with the emergency timeouts (they are defined below).
# If the memcap is reached, the engine will try to prune flows
# with the default timeouts. If it doens't find a flow to prune, it will set
# the emergency bit and it will try again with more agressive timeouts.
# If that doesn't work, then it will try to kill the last time seen flows
# not in use.
# The memcap can be specified in kb, mb, gb.  Just a number indicates it's
# in bytes.

flow:
  memcap: {{ flow_memcap }}
  hash-size: {{ flow_hash_size }}
  prealloc: {{ flow_prealloc }}
  emergency-recovery: {{ flow_emergency_recovery }}
  prune-flows: {{ flow_prune_flows }}
  managers: {{ flow_managers }} # default to one flow manager
  recyclers: {{ flow_recyclers }} # default to one flow recycler thread

# Specific timeouts for flows. Here you can specify the timeouts that the
# active flows will wait to transit from the current state to another, on each
# protocol. The value of "new" determine the seconds to wait after a hanshake or
# stream startup before the engine free the data of that flow it doesn't
# change the state to established (usually if we don't receive more packets
# of that flow). The value of "established" is the amount of
# seconds that the engine will wait to free the flow if it spend that amount
# without receiving new packets or closing the connection. "closed" is the
# amount of time to wait after a flow is closed (usually zero).
#
# There's an emergency mode that will become active under attack circumstances,
# making the engine to check flow status faster. This configuration variables
# use the prefix "emergency-" and work similar as the normal ones.
# Some timeouts doesn't apply to all the protocols, like "closed", for udp and
# icmp.

flow-timeouts:

  default:
    new: {{ timeout_default_new }}
    established: {{ timeout_default_established }}
    closed: {{ timeout_default_closed }}
    emergency-new: {{ timeout_default_emergency_new }}
    emergency-established: {{ timeout_default_emergency_established }}
    emergency-closed: {{ timeout_default_emergency_closed }}
  tcp:
    new: {{ timeout_tcp_new }}
    established: {{ timeout_tcp_established }}
    closed: {{ timeout_tcp_closed }}
    emergency-new: {{ timeout_tcp_emergency_new }}
    emergency-established: {{ timeout_tcp_emergency_established }}
    emergency-closed: {{ timeout_tcp_emergency_closed }}
  udp:
    new: {{ timeout_udp_new }}
    established: {{ timeout_udp_established }}
    emergency-new: {{ timeout_udp_emergency_new }}
    emergency-established: {{ timeout_udp_emergency_established }}
  icmp:
    new: {{ timeout_icmp_new }}
    established: {{ timeout_icmp_established }}
    emergency-new: {{ timeout_icmp_emergency_new }}
    emergency-established: {{ timeout_icmp_emergency_established }}

# Stream engine settings. Here the TCP stream tracking and reaasembly
# engine is configured.
#
# stream:
#   memcap: 32mb                # Can be specified in kb, mb, gb.  Just a
#                               # number indicates it's in bytes.
#   checksum-validation: yes    # To validate the checksum of received
#                               # packet. If csum validation is specified as
#                               # "yes", then packet with invalid csum will not
#                               # be processed by the engine stream/app layer.
#                               # Warning: locally generated trafic can be
#                               # generated without checksum due to hardware offload
#                               # of checksum. You can control the handling of checksum
#				# on a per-interface basis via the 'checksum-checks'
#				# option
#   max-sessions: 262144        # 256k concurrent sessions
#   prealloc-sessions: 32768    # 32k sessions prealloc'd
#   midstream: false            # don't allow midstream session pickups
#   async-oneside: false        # don't enable async stream handling
#   inline: no                  # stream inline mode
#
#   reassembly:
#     memcap: 64mb              # Can be specified in kb, mb, gb.  Just a number
#                               # indicates it's in bytes.
#     depth: 1mb                # Can be specified in kb, mb, gb.  Just a number
#                               # indicates it's in bytes.
#     toserver-chunk-size: 2560 # inspect raw stream in chunks of at least
#                               # this size.  Can be specified in kb, mb,
#                               # gb.  Just a number indicates it's in bytes.
#     toclient-chunk-size: 2560 # inspect raw stream in chunks of at least
#                               # this size.  Can be specified in kb, mb,
#                               # gb.  Just a number indicates it's in bytes.

stream:
  memcap: {{ stream_memcap }}
  prealloc-sessions: {{ stream_prealloc_sessions }}
  checksum-validation: no       # reject wrong csums
  inline: no                    # no inline mode
  midstream: false
  async-oneside: false
  reassembly:
    memcap: {{ stream_reassembly_memcap }}
    depth: {{ stream_reassembly_depth }}                  # reassemble 1mb into a stream
    toserver-chunk-size: {{ stream_reassembly_toserver_chunk_size }}
    toclient-chunk-size: {{ stream_reassembly_toclient_chunk_size }}
    randomize-chunk-size: yes
    chunk-prealloc: {{ stream_reassembly_chunk_prealloc }}
    segments:
{% for item in segments %}
      - size: {{ item.size }}
        prealloc: {{ item.prealloc }}
{% endfor %}


# Host table:
#
# Host table is used by tagging and per host thresholding subsystems.
#
host:
  hash-size: 4096
  prealloc: 1000
  memcap: 32mb

# Logging configuration.  This is not about logging IDS alerts, but
# IDS output about what its doing, errors, etc.
logging:

  # The default log level, can be overridden in an output section.
  # Note that debug level logging will only be emitted if Suricata was
  # compiled with the --enable-debug configure option.
  #
  # This value is overriden by the SC_LOG_LEVEL env var.
  default-log-level: info

  # The default output format.  Optional parameter, should default to
  # something reasonable if not provided.  Can be overriden in an
  # output section.  You can leave this out to get the default.
  #
  # This value is overriden by the SC_LOG_FORMAT env var.

  # A regex to filter output.  Can be overridden in an output section.
  # Defaults to empty (no filter).
  #
  # This value is overriden by the SC_LOG_OP_FILTER env var.
  default-output-filter:

  # Define your logging outputs.  If none are defined, or they are all
  # disabled you will get the default - console output.
  outputs:
  - console:
      enabled: yes
  - file:
      enabled: yes
      filename: /var/log/nsm/suricata.log

# Set the default rule path here to search for the files.
# if not set, it will look at the current working dir
default-rule-path: /etc/nsm/rules/
rule-files:
{% for item in rule_files %}
  - {{ item }}
{% endfor %}

classification-file: /etc/nsm/classification.config
reference-config-file: /etc/nsm/reference.config

# Holds variables that would be used by the engine.
vars:

  # Holds the address group vars that would be passed in a Signature.
  # These would be retrieved during the Signature address parsing stage.
  address-groups:

    PROXY_SERVERS: "{{ proxy_servers }}"

    HOME_NET: "{{ home_net }}"

    REALLY_EXTERNAL_NET: "!$HOME_NET"

    WIN_AD: "{{ win_ad }}"

    EXTERNAL_NET: "{{ external_net }}"

    MULTICAST_NET: "{{ multicast_net }}"

    RDP_SERVERS: "{{ rdp_servers }}"

    IRC_CLIENTS: "{{ irc_clients }}"

    HTTP_SERVERS: "{{ http_servers }}"

    SMTP_SERVERS: "{{ smtp_servers }}"

    SQL_SERVERS: "{{ sql_servers }}"

    DNS_SERVERS: "{{ dns_servers }}"

    TELNET_SERVERS: "{{ telnet_servers }}"

    AIM_SERVERS: "{{ aim_servers }}"

    DNP3_SERVER: "{{ dnp3_server }}"

    DNP3_CLIENT: "{{ dnp3_client }}"

    MODBUS_CLIENT: "{{ modbus_client }}"

    MODBUS_SERVER: "{{ modbus_server }}"

    ENIP_CLIENT: "{{ enip_client }}"

    ENIP_SERVER: "{{ enip_server }}"

  # Holds the port group vars that would be passed in a Signature.
  # These would be retrieved during the Signature port parsing stage.

  port-groups:

    HTTP_PORTS: "{{ http_ports }}"

    SHELLCODE_PORTS: "{{ shellcode_ports }}"

    ORACLE_PORTS: "{{ oracle_ports }}"

    SSH_PORTS: "{{ ssh_ports }}"

    DNP3_PORTS: "{{ dnp3_ports }}"

# Set the order of alerts bassed on actions
# The default order is pass, drop, reject, alert
action-order:
  - pass
  - drop
  - reject
  - alert

# Host specific policies for defragmentation and TCP stream
# reassembly.  The host OS lookup is done using a radix tree, just
# like a routing table so the most specific entry matches.
host-os-policy:
  # Make the default policy windows.
  windows: []
  bsd: []
  bsd-right: []
  old-linux: []
  linux: [0.0.0.0/0]
  old-solaris: []
  solaris: []
  hpux10: []
  hpux11: []
  irix: []
  macos: []
  vista: []
  windows2k3: []

# Limit for the maximum number of asn1 frames to decode (default 256)
asn1-max-frames: 256

# When run with the option --engine-analysis, the engine will read each of
# the parameters below, and print reports for each of the enabled sections
# and exit.  The reports are printed to a file in the default log dir
# given by the parameter "default-log-dir", with engine reporting
# subsection below printing reports in its own report file.
engine-analysis:
  # enables printing reports for fast-pattern for every rule.
  rules-fast-pattern: yes
  # enables printing reports for each rule
  rules: yes

#recursion and match limits for PCRE where supported
pcre:
  match-limit: 3500
  match-limit-recursion: 1500

# Configure the app-layer parsers. The protocols section details each
# protocol.
#
# The option "enabled" takes 3 values - "yes", "no", "detection-only".
# "yes" enables both detection and the parser, "no" disables both, and
# "detection-only" enables protocol detection only (parser disabled).
app-layer:
  protocols:
    tls:
      enabled: yes
      detection-ports:
        dp: 443

      #no-reassemble: yes
    dcerpc:
      enabled: yes
    ftp:
      enabled: yes
    ssh:
      enabled: yes
    smtp:
      enabled: yes
      # Configure SMTP-MIME Decoder
      mime:
        # Decode MIME messages from SMTP transactions
        # (may be resource intensive)
        # This field supercedes all others because it turns the entire
        # process on or off
        decode-mime: yes

        # Decode MIME entity bodies (ie. base64, quoted-printable, etc.)
        decode-base64: yes
        decode-quoted-printable: yes

        # Maximum bytes per header data value stored in the data structure
        # (default is 2000)
        header-value-depth: 2000

        # Extract URLs and save in state data structure
        extract-urls: yes
        # Set to yes to compute the md5 of the mail body. You will then
        # be able to journalize it.
        body-md5: no
      # Configure inspected-tracker for file_data keyword
      inspected-tracker:
        content-limit: 100000
        content-inspect-min-size: 32768
        content-inspect-window: 4096
    imap:
      enabled: detection-only
    msn:
      enabled: detection-only
    smb:
      enabled: yes
      detection-ports:
        dp: 139
    # Note: Modbus probe parser is minimalist due to the poor significant field
    # Only Modbus message length (greater than Modbus header length)
    # And Protocol ID (equal to 0) are checked in probing parser
    # It is important to enable detection port and define Modbus port
    # to avoid false positive
    modbus:
      # How many unreplied Modbus requests are considered a flood.
      # If the limit is reached, app-layer-event:modbus.flooded; will match.
      #request-flood: 500

      enabled: no
      detection-ports:
        dp: 502
      # According to MODBUS Messaging on TCP/IP Implementation Guide V1.0b, it
      # is recommended to keep the TCP connection opened with a remote device
      # and not to open and close it for each MODBUS/TCP transaction. In that
      # case, it is important to set the depth of the stream reassembling as
      # unlimited (stream.reassembly.depth: 0)
    # smb2 detection is disabled internally inside the engine.
    #smb2:
    #  enabled: yes
    dns:
      # memcaps. Globally and per flow/state.
      global-memcap: {{ dns_global_memcap }}
      state-memcap: {{ dns_state_memcap }}

      # How many unreplied DNS requests are considered a flood.
      # If the limit is reached, app-layer-event:dns.flooded; will match.
      #request-flood: 500

      tcp:
        enabled: yes
        detection-ports:
          dp: 53
      udp:
        enabled: yes
        detection-ports:
          dp: 53
    http:
      enabled: yes
      memcap: {{ http_memcap }}


###########################################################################
# Configure libhtp.
#
#
# default-config:           Used when no server-config matches
#   personality:            List of personalities used by default
#   request-body-limit:     Limit reassembly of request body for inspection
#                           by http_client_body & pcre /P option.
#   response-body-limit:    Limit reassembly of response body for inspection
#                           by file_data, http_server_body & pcre /Q option.
#   double-decode-path:     Double decode path section of the URI
#   double-decode-query:    Double decode query section of the URI
#
# server-config:            List of server configurations to use if address matches
#   address:                List of ip addresses or networks for this block
#   personalitiy:           List of personalities used by this block
#   request-body-limit:     Limit reassembly of request body for inspection
#                           by http_client_body & pcre /P option.
#   response-body-limit:    Limit reassembly of response body for inspection
#                           by file_data, http_server_body & pcre /Q option.
#   double-decode-path:     Double decode path section of the URI
#   double-decode-query:    Double decode query section of the URI
#
# Currently Available Personalities:
#   Minimal
#   Generic
#   IDS (default)
#   IIS_4_0
#   IIS_5_0
#   IIS_5_1
#   IIS_6_0
#   IIS_7_0
#   IIS_7_5
#   Apache
#   Apache_2_2
###########################################################################
libhtp:

   default-config:
     personality: IDS
     # Can be specified in kb, mb, gb.  Just a number indicates
     # it's in bytes.
     request-body-limit: "{{ request_body_limit }}"
     response-body-limit: "{{ response_body_limit }}"
     double-decode-path: "{{ double_decode_path }}"
     double-decode-query: "{{ double_decode_query }}"
     request-body-minimal-inspect-size: "{{ request_body_minimal_inspect_size }}"
     request-body-inspect-window: "{{ request_body_inspect_window }}"
     response-body-minimal-inspect-size: "{{ response_body_minimal_inspect_size }}"
     response-body-inspect-window: "{{ response_body_inspect_window }}"

##### Set proper personality for your net #######
#   server-config:
#     - apache:
#         address: [192.168.1.0/24, 127.0.0.0/8, "::1"]
#         personality: Apache_2_2
#         # Can be specified in kb, mb, gb.  Just a number indicates
#         # it's in bytes.
#         request-body-limit: 0
#         response-body-limit: 0
#         double-decode-path: no
#         double-decode-query: no
#
#     - iis7:
#         address:
#           - 192.168.0.0/24
#           - 192.168.10.0/24
#         personality: IIS_7_0
#         # Can be specified in kb, mb, gb.  Just a number indicates
#         # it's in bytes.
#         request-body-limit: 0
#         response-body-limit: 0
#         double-decode-path: no
#         double-decode-query: no

# Suricata core dump configuration. Limits the size of the core dump file to
# approximately max-dump. The actual core dump size will be a multiple of the
# page size. Core dumps that would be larger than max-dump are truncated. On
# Linux, the actual core dump size may be a few pages larger than max-dump.
# Setting max-dump to 0 disables core dumping.
# Setting max-dump to 'unlimited' will give the full core dump file.
# On 32-bit Linux, a max-dump value >= ULONG_MAX may cause the core dump size
# to be 'unlimited'.

coredump:
  max-dump: unlimited

